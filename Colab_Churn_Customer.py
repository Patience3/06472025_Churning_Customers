# -*- coding: utf-8 -*-
"""Sombang_Churn_Assignmnet3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TawOwtfmQRDurdOHp05P3Cq_yQZJrnyA

##Data loading and display
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

df=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Intro To AI/Project/CustomerChurn_dataset.csv')

df.head()

df.info()

"""##Data Preprocessing using one hot encoding for binary categorie

Droping the customer id by domain knowledge
"""

df.drop(['customerID'], axis=1, inplace=True)

df['SeniorCitizen'] = df['SeniorCitizen'].map({0:"No", 1:"Yes"})
df.head()

df_removed_cust=df.copy()

non_numeric_columns = ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod','Churn','SeniorCitizen']

# One-hot encode non-numeric features
df_encoded = pd.get_dummies(df[non_numeric_columns])


# Convert 'TotalCharges' to numeric
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')  # coerce to handle conversion errors

# Concatenate the one-hot encoded DataFrame and the original DataFrame using the original index
df = pd.concat([df, df_encoded], axis=1)

# Drop the original non-numeric columns
df.drop(non_numeric_columns, axis=1, inplace=True)

# Display the result
df.head()

df.info()

"""##Feature selection

###Performing Feature importance on data
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.impute import SimpleImputer

# 'df' is your DataFrame with the one-hot encoded features
# 'Churn' is the target variable

# Handle missing values by imputing with mean for the entire DataFrame
imputer = SimpleImputer(strategy='mean')
df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Extract X and y
X = df_imputed.drop(columns=['Churn_Yes', 'Churn_No'], axis=1)
y = df_imputed[['Churn_Yes']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model
rf_classifier.fit(X_train, y_train)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame with feature names and their importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(20, 20))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')
plt.title('Feature Importances')
plt.show()

df

"""From the above feature importance, I will select some feature, and I will perform a corelation to select some features that the biases of the random forest based feature importance may have missed."""

list_of_features_from_feature_importance=[
"tenure", "MonthlyCharges", "TotalCharges", "gender_Female", "gender_Male", "Partner_Yes","Dependents_Yes",
"PhoneService_No", "MultipleLines_No", "InternetService_DSL", "InternetService_Fiber optic",
"InternetService_No", "OnlineSecurity_No", "OnlineBackup_No", "DeviceProtection_No", "TechSupport_No",
 "Contract_Month-to-month", "Contract_Two year", "PaperlessBilling_No", "PaperlessBilling_Yes", "PaymentMethod_Electronic check"]

"""###Performing correlation for validation and continual selection"""

df_corr= df.corr()
df_corr['Churn_Yes'].sort_values(ascending=False)

"""From the above corelation, values like gender are"""

df

temp = df_corr[abs(df_corr['Churn_Yes']) > 0.15]['Churn_Yes']
df_correlation = df[temp.index]
df_correlation.info()

"""Though dependents is not included in the feture importance, it seems significant to include the dependents as there is a probability of people with more dependents to keep them busy eith the internet."""

df_corr= df_correlation.corr()
df_corr['Churn_Yes'].sort_values(ascending=False)

"""From the above correlation output, some values like gender are direct opposite corelation with their counterpart gender female. Thus if only one is considered (gender_male) and the response is no, it automatically implies gender female. Thus I will remove their counterparts. Thus I will consider one, to capture bothe poles. Also, with features that are dependent on internet service like streaming, if the internet service is no, they automatically become irrelevant to my dataset."""

list_of_features_from_corelation = ["Churn_Yes", "Contract_Month-to-month", "OnlineSecurity_No", "TechSupport_No", "InternetService_Fiber optic", "PaymentMethod_Electronic check", "OnlineBackup_No", "DeviceProtection_No", "MonthlyCharges", "PaperlessBilling_Yes", "SeniorCitizen_Yes", "Partner_No", "Dependents_Yes", "TechSupport_Yes", "OnlineSecurity_Yes", "Contract_One year", "PaperlessBilling_No", "TotalCharges", "InternetService_No", "Contract_Two year", "tenure"]

"""I will write code, that would create a list of the element that are both in the feature importance list, and the correlation selection list to get a faire understandijng of my distribution. After that, I will display the remaining features for reconsideration.

###Final features phase
"""

# Find the intersection of the two lists
common_features = set(list_of_features_from_corelation) & set(list_of_features_from_feature_importance)

# Display the common features
print("Common Features:\n\n", common_features)

# Find features that are not present in the intersection
not_found_in_intersection = set(list_of_features_from_corelation) ^ set(list_of_features_from_feature_importance)

# Display features not found in the intersection
print("\n\nFeatures not found in the intersection:\n", not_found_in_intersection)

df.info()

"""Current ste of DF"""

# Create a DataFrame with common features
df_current = df[['tenure', 'TotalCharges', 'OnlineSecurity_No', 'Contract_Two year', 'InternetService_No',
                   'PaymentMethod_Electronic check', 'InternetService_Fiber optic', 'PaperlessBilling_Yes',
                   'TechSupport_No', 'PaperlessBilling_No', 'MonthlyCharges', 'Dependents_Yes',
                   'Contract_Month-to-month', 'DeviceProtection_No', 'OnlineBackup_No']]

# Add additional features to the DataFrame
additional_features = ['SeniorCitizen_Yes', 'gender_Male', 'TechSupport_Yes', 'InternetService_DSL',
                       'Partner_Yes', 'OnlineSecurity_Yes', 'PhoneService_No', 'Partner_No',
                       'Contract_One year', 'Churn_Yes']

df = pd.concat([df_current, df[additional_features]], axis=1)

# Display the new DataFrame
df.info()

"""##EDA ANALYSIS"""

# Replace 'Churn_Yes' with the actual one-hot encoded column for 'Churn' being 'Yes'
churned_customers = df[df['Churn_Yes'] == 1]

# Box plots for selected numerical features
plt.figure(figsize=(8, 5))

# Monthly Charges, Tenure, Total Charges
plt.subplot(1, 3, 1)
sns.boxplot(x='Churn_Yes', y='MonthlyCharges', data=df)
plt.title('Monthly Charges by Churn')

plt.subplot(1, 3, 2)
sns.boxplot(x='Churn_Yes', y='tenure', data=df)
plt.title('Tenure by Churn')

plt.subplot(1, 3, 3)
sns.boxplot(x='Churn_Yes', y='TotalCharges', data=df)
plt.title('Total Charges by Churn')

plt.tight_layout()
plt.show()

# Pair plot for all other features against Churn_Yes
sns.set(style="whitegrid", font_scale=1.2)
pair_plot_df = df.drop(['MonthlyCharges', 'tenure', 'TotalCharges'], axis=1)  # Exclude numerical features from pair plot

plt.figure(figsize=(25, 25))

# Calculate the number of rows and columns dynamically
num_features = pair_plot_df.shape[1]
num_cols = 4
num_rows = -(-num_features // num_cols)  # Ceiling division to calculate the number of rows

# Loop through the remaining features
for i, feature in enumerate(pair_plot_df.columns):
    ax = plt.subplot(num_rows, num_cols, i+1)
    sns.countplot(x=feature, hue='Churn_Yes', data=df)
    plt.title(f'Countplot of {feature} by Churn Yes')

    # Annotate with percentages
    total = float(len(df))
    for p in ax.patches:
        height = p.get_height()
        ax.text(p.get_x() + p.get_width() / 2., height + 0.1, f'{(height / total * 100):.2f}%', ha="center")

plt.tight_layout()
plt.show()

"""From the above gaphic representations, I can conclude that  a person  

1.   No two year contract
2.   with phone service
3.   With no partners
4. with no online security
5. With internet service that is not DSL
6. with no tech support
7. with a month to month contract
8. with no dependents
9. with no device protection
10. with no online back up
11. with fibre optic internet service
12. With internet service
15. With electronic check payment method
16. with paperless billing

 Are those who churn more

##Selecting features for the progression
From the above grpahic analysis, Gender seems to have no relationship with churn. Thus I will renarrow my features thus.
"""

df_progression = df[['tenure', 'OnlineSecurity_No', 'InternetService_No', 'PaymentMethod_Electronic check', 'InternetService_Fiber optic', 'PaperlessBilling_Yes', 'TechSupport_No', 'MonthlyCharges', 'Dependents_Yes', 'Contract_Month-to-month', 'DeviceProtection_No', 'OnlineBackup_No', 'SeniorCitizen_Yes', 'TechSupport_Yes', 'InternetService_DSL', 'Partner_Yes', 'OnlineSecurity_Yes', 'PhoneService_No', 'Contract_One year', 'Churn_Yes']]

y = df_progression['Churn_Yes']
X = df_progression.drop(['Churn_Yes'], axis=1)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into training (80%), validation (10%), and testing (10%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)
X.info()

"""###Feature validation

##Training and Accuracy
"""

import keras
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from keras.utils import to_categorical


# from keras.optimizers import Adam

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Keras Functional API model
input_layer = Input(shape=(X_train.shape[1],))
hidden_layer_1 = Dense(32, activation='relu')(input_layer)
hidden_layer_2 = Dense(24, activation='relu')(hidden_layer_1)
hidden_layer_3 = Dense(12, activation='relu')(hidden_layer_2)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_3)

model = Model(inputs=input_layer, outputs=output_layer)

model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

_, accuracy = model.evaluate(X_train, y_train)
accuracy*100

loss, accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy*100:.4f}')

"""##Hyper parameter tunning"""

!pip install keras-tuner

import keras_tuner
from tensorflow import keras

from tensorflow.keras.layers import Input, Dense, Dropout

def build_model(hp):
    inputs = Input(shape=(X_train.shape[1],))
    x = inputs

    # Tune the number of hidden layers and units
    for i in range(hp.Int('num_hidden_layers', min_value=1, max_value=4)):
        x = Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=96, step=32),
                  activation=hp.Choice(f'activation_{i}', values=['relu', 'tanh']))(x)
        x = Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.5, step=0.1))(x)

    outputs = Dense(1, activation='sigmoid')(x)

    # Tune the learning rate
    learning_rate = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")

    model = keras.Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )

    return model

"""###test the model"""

build_model(keras_tuner.HyperParameters())

tuner = keras_tuner.Hyperband(
  hypermodel=build_model,
  objective='val_accuracy',
  max_epochs=100,
  factor=3,
  directory='tuning_dir',
  project_name='samples')

tuner.search(X_train, y_train, epochs=30 ,validation_data=(X_test, y_test))

tuner.search_space_summary()

tuner.results_summary()

best_model = tuner.get_best_models(num_models=2)[0]
best_model.summary()

test_accuracy = best_model.evaluate(X_test, y_test)[1]
print(f"Test Accuracy: {test_accuracy:.4f}")

print("Best Hyperparameters:\n","""

        Trial 0243 summary
        Hyperparameters:
        num_hidden_layers: 1
        units_0: 32
        activation_0: tanh
        lr: 0.00175004788390259
        units_1: 32
        activation_1: tanh
        units_2: 32
        activation_2: tanh
        units_3: 96
        activation_3: tanh
        tuner/epochs: 34
        tuner/initial_epoch: 0
        tuner/bracket: 1
        tuner/round: 0
        Score: 0.8311688303947449
      """)

"""##Deployment of Model"""

import streamlit as st
import tensorflow as ts
import keras as k

# Save the scalers to a file
import pickle
with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

"""Saving the model"""

# Save the TensorFlow model
best_model.save("best_model.h5")

"""###Find the rest in my Python deployment file

<Papa Yaw Boampong Wireko> (<16/Nov/2023>) <Final Presentation/Intro to AI> (<ANN>) [<>]
"""